{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1>Disaster Tweets Classification</h1>\n\n<h4 style='text-align: justify;'>\nThis notebook is mainly to preprocess the dataset. At the beginning, I tested the accuracy that can be reached by using trivial methods to predict the output. Afterwards, the data is procesed to be use by BERT. The machine learning models are notshown in this notebook because I focused on the models on another notebook. Neural networks with BERT was the main interest, yet other algorithm were used too.\n<br/>\nFor the machine learning models, check the link below:\n    <a href=''>Machine Learning Notebook</a>\n</h4>","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-10-02T17:13:15.087744Z","iopub.execute_input":"2021-10-02T17:13:15.088127Z","iopub.status.idle":"2021-10-02T17:13:15.097731Z","shell.execute_reply.started":"2021-10-02T17:13:15.088096Z","shell.execute_reply":"2021-10-02T17:13:15.096756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nimport nltk\nimport tensorflow_hub as hub\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom collections import OrderedDict,Counter\nimport sklearn\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.manifold import TSNE","metadata":{"execution":{"iopub.status.busy":"2021-10-02T17:13:15.124104Z","iopub.execute_input":"2021-10-02T17:13:15.124643Z","iopub.status.idle":"2021-10-02T17:13:22.992394Z","shell.execute_reply.started":"2021-10-02T17:13:15.124599Z","shell.execute_reply":"2021-10-02T17:13:22.991429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_splits=7\nrandom_state=27","metadata":{"execution":{"iopub.status.busy":"2021-10-02T17:13:22.993657Z","iopub.execute_input":"2021-10-02T17:13:22.994068Z","iopub.status.idle":"2021-10-02T17:13:22.997596Z","shell.execute_reply.started":"2021-10-02T17:13:22.994021Z","shell.execute_reply":"2021-10-02T17:13:22.996705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2><b>Functions</b></h2>\n\n<h4>\nHere some useful functions are created\n</h4>","metadata":{}},{"cell_type":"code","source":"def get_keyword_mean(train_df,val_df,column='keyword',target='target'):\n    aggregate_column=column+'_mean'\n    train_df[aggregate_column]=train_df.groupby(column)[target].transform('mean')\n    \n    val_df = val_df.merge(\n                train_df[[column, aggregate_column]].drop_duplicates(),\n                on=column,\n                how=\"left\",\n            )\n    return train_df,val_df","metadata":{"execution":{"iopub.status.busy":"2021-10-02T17:13:22.999208Z","iopub.execute_input":"2021-10-02T17:13:22.999627Z","iopub.status.idle":"2021-10-02T17:13:23.011682Z","shell.execute_reply.started":"2021-10-02T17:13:22.999576Z","shell.execute_reply":"2021-10-02T17:13:23.010467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def cv_datasets(df,model,regex='^BERT',target_column='target',use_keyword=False,model_function=None,compile_dic={},**kwargs):\n    '''\n    df: Dataframe to divide the data.\n    model: Model used to fit and evaluate the data.\n    regex: Regex used to pick a column. By default, it searches for the columns starting with BERT.\n    target_column: Name of the column containing the output.\n    compile_dic: It is only used for keras models\n    '''\n    kfold=KFold(n_splits=n_splits,shuffle=True,random_state=random_state)\n    results={'Total':0}\n    i=0\n    \n    #iterate through the folds\n    for train_index,val_index in kfold.split(df):\n        \n        if 'keras' in str(type(model)):\n            #keras\n            model_copy=model_function()\n            model_copy.compile(**compile_dic)\n    \n            model_copy.summary()\n        else:\n            #sklearn\n            model_copy= sklearn.base.clone(model)\n            \n        train_df=df.iloc[train_index]\n        val_df=df.iloc[val_index]\n        \n        if use_keyword:\n            train_df,val_df=get_keyword_mean(train_df,val_df)\n            regex+='|^keyword_'\n        \n        X_train=train_df.filter(regex=regex,axis=1)\n        y_train=train_df[target_column]\n        \n        X_val=val_df.filter(regex=regex,axis=1)\n        y_val=val_df[target_column]\n        \n        #print(X_train)\n        X_train=X_train.copy().values\n        y_train=y_train.copy().values\n        \n        X_val=X_val.copy().values\n        y_val=y_val.copy().values\n        \n        'The problem with keras mmodels is the copy itself'\n        model_copy.fit(X_train,y_train,**kwargs)\n        \n        predicted=model_copy.predict(X_val)\n        \n        metric=confusion_matrix(y_val,predicted)\n        \n        tn, fp, fn, tp = metric.ravel()\n        \n        results_matrix=metric\n        results['Batch '+str(i)]=results_matrix \n        results['Total']+=np.array(results_matrix)\n        i+=1\n        \n        print(f'{i}th run:\\naccuracy: {(tp+tn)/(tn+fp+fn+tp)}\\nprecision:{tp/(tp+fp)}\\nrecall:{tp/(tp+fn)}\\n')\n    \n    \n    tn, fp, fn, tp = results['Total'].ravel()\n    \n    print(f'Total:\\naccuracy: {(tp+tn)/(tn+fp+fn+tp)}\\nprecision:{tp/(tp+fp)}\\nrecall:{tp/(tp+fn)}\\n')\n    \n    return results\n        ","metadata":{"execution":{"iopub.status.busy":"2021-10-02T17:13:23.013772Z","iopub.execute_input":"2021-10-02T17:13:23.014158Z","iopub.status.idle":"2021-10-02T17:13:23.030099Z","shell.execute_reply.started":"2021-10-02T17:13:23.014126Z","shell.execute_reply":"2021-10-02T17:13:23.02887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fit_and_predict(model,train_df,test_df,use_keyword=False,regex='^BERT',target_column='target',**kwargs):\n    if use_keyword:\n        train_df,test_df=get_keyword_mean(train_df,test_df)\n        regex+='|^keyword_'\n\n    X_train=train_df.filter(regex=regex,axis=1)\n    y_train=train_df[target_column]\n\n    X_test=test_df.filter(regex=regex,axis=1)\n    \n    model.fit(X_train,y_train,**kwargs)\n    \n    test_df['predictions']=(model.predict(X_test)>0.5).astype(int)\n    \n    return test_df","metadata":{"execution":{"iopub.status.busy":"2021-10-02T17:13:23.031531Z","iopub.execute_input":"2021-10-02T17:13:23.031837Z","iopub.status.idle":"2021-10-02T17:13:23.046095Z","shell.execute_reply.started":"2021-10-02T17:13:23.031806Z","shell.execute_reply":"2021-10-02T17:13:23.045177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_submission(test_df,predictions_column='predictions',name='submission'):\n#     new_submission=pd.DataFrame()\n#     new_submission['id']=test_df['id']\n#     new_submission['target']=predictions\n#     new_submission=new_submission.set_index('id')\n    \n    new_submission=test_df[['id',predictions_column]].set_index('id').rename(columns={predictions_column:'target'})\n    print(f'saving the results in {name}.csv')\n    new_submission.to_csv(name+'.csv')\n    print('finished saving')\n    \n    return new_submission","metadata":{"execution":{"iopub.status.busy":"2021-10-02T17:13:23.047504Z","iopub.execute_input":"2021-10-02T17:13:23.047967Z","iopub.status.idle":"2021-10-02T17:13:23.060543Z","shell.execute_reply.started":"2021-10-02T17:13:23.047919Z","shell.execute_reply":"2021-10-02T17:13:23.059417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_results(df,model,name='submission',regex='^BERT',target_column='target',use_keyword=False,compile_dic={},model_function=None,**kwargs):\n    df['keyword']=df['keyword'].fillna('unk')\n    df['location']=df['location'].fillna('unk_location')\n    \n    train_df=df[df['trainable']==1]\n    test_df=df[~(df['trainable']==1)]\n    \n    results=cv_datasets(train_df,model,regex=regex,target_column=target_column,model_function=model_function,use_keyword=use_keyword,**kwargs)\n    \n    test_df=fit_and_predict(model,train_df,test_df,use_keyword=use_keyword,regex=regex,target_column=target_column,**kwargs)\n    \n    test_df['predictions']=test_df['predictions'].values>0.5\n    test_df['predictions']=test_df['predictions'].astype(int)\n    \n    submission=generate_submission(test_df,predictions_column='predictions',name=name)\n    \n    return results,test_df,submission","metadata":{"execution":{"iopub.status.busy":"2021-10-02T17:13:23.062145Z","iopub.execute_input":"2021-10-02T17:13:23.062441Z","iopub.status.idle":"2021-10-02T17:13:23.07442Z","shell.execute_reply.started":"2021-10-02T17:13:23.062414Z","shell.execute_reply":"2021-10-02T17:13:23.073306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=pd.read_csv('/kaggle/input/bert-features/BERT_processed.csv')\nresults,test_df,submission=generate_results(df,model=LogisticRegression(solver='sag'),use_keyword=True)","metadata":{"execution":{"iopub.status.busy":"2021-10-02T17:13:23.077839Z","iopub.execute_input":"2021-10-02T17:13:23.078554Z","iopub.status.idle":"2021-10-02T17:13:30.845512Z","shell.execute_reply.started":"2021-10-02T17:13:23.0785Z","shell.execute_reply":"2021-10-02T17:13:30.84419Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=pd.read_csv('/kaggle/input/bert-features/BERT_processed.csv')\n\nrf_parameters={\n    'n_estimators':300,\n    'max_depth':20,\n    'min_samples_split':80,\n    'min_samples_leaf':30,\n    'min_weight_fraction_leaf':0.0,\n    'max_features':'auto',\n    'max_leaf_nodes':40,\n    'min_impurity_decrease':0.0,\n    'min_impurity_split':None,\n}\nresults,test_df,submission=generate_results(df,model=RandomForestClassifier(**rf_parameters),use_keyword=True)","metadata":{"execution":{"iopub.status.busy":"2021-09-19T19:03:31.538145Z","iopub.execute_input":"2021-09-19T19:03:31.53863Z","iopub.status.idle":"2021-09-19T19:04:55.45513Z","shell.execute_reply.started":"2021-09-19T19:03:31.538567Z","shell.execute_reply":"2021-09-19T19:04:55.454081Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Best till now\ndf=pd.read_csv('/kaggle/input/bert-features/BERT_processed.csv')\nresults,test_df,submission=generate_results(df,model=SVC(C=5),use_keyword=True)","metadata":{"execution":{"iopub.status.busy":"2021-09-19T19:10:08.460614Z","iopub.execute_input":"2021-09-19T19:10:08.46101Z","iopub.status.idle":"2021-09-19T19:11:03.673293Z","shell.execute_reply.started":"2021-09-19T19:10:08.460976Z","shell.execute_reply":"2021-09-19T19:11:03.67233Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=pd.read_csv('/kaggle/input/bert-features/BERT_processed.csv')\nresults,test_df,submission=generate_results(df,model=KNeighborsClassifier(200,weights='distance',p=2),use_keyword=True)","metadata":{"execution":{"iopub.status.busy":"2021-09-19T12:37:26.694448Z","iopub.execute_input":"2021-09-19T12:37:26.695085Z","iopub.status.idle":"2021-09-19T12:37:46.991366Z","shell.execute_reply.started":"2021-09-19T12:37:26.695041Z","shell.execute_reply":"2021-09-19T12:37:46.990541Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results,test_df,submission\nresults['Total']","metadata":{"execution":{"iopub.status.busy":"2021-09-19T12:37:15.590015Z","iopub.status.idle":"2021-09-19T12:37:15.590538Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tn, fp, fn, tp = results['Total'].ravel()\n(tn+tp)/(tn+fp+fn+tp)","metadata":{"execution":{"iopub.status.busy":"2021-09-19T12:37:15.591288Z","iopub.status.idle":"2021-09-19T12:37:15.591629Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2><b>Get The Data</b></h2>","metadata":{}},{"cell_type":"code","source":"train_df=pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ntest_df=pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\nsample_submission=pd.read_csv('/kaggle/input/nlp-getting-started/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2021-10-02T17:14:09.841914Z","iopub.execute_input":"2021-10-02T17:14:09.842613Z","iopub.status.idle":"2021-10-02T17:14:09.943293Z","shell.execute_reply.started":"2021-10-02T17:14:09.842578Z","shell.execute_reply":"2021-10-02T17:14:09.942134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission","metadata":{"execution":{"iopub.status.busy":"2021-10-02T17:14:10.697176Z","iopub.execute_input":"2021-10-02T17:14:10.697656Z","iopub.status.idle":"2021-10-02T17:14:10.718441Z","shell.execute_reply.started":"2021-10-02T17:14:10.697618Z","shell.execute_reply":"2021-10-02T17:14:10.717223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df[train_df['keyword'].notnull()]","metadata":{"execution":{"iopub.status.busy":"2021-10-02T17:14:11.014696Z","iopub.execute_input":"2021-10-02T17:14:11.015091Z","iopub.status.idle":"2021-10-02T17:14:11.035755Z","shell.execute_reply.started":"2021-10-02T17:14:11.01506Z","shell.execute_reply":"2021-10-02T17:14:11.034654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df['keyword']=test_df['keyword'].fillna('unk')\ntest_df","metadata":{"execution":{"iopub.status.busy":"2021-10-02T17:14:11.632408Z","iopub.execute_input":"2021-10-02T17:14:11.632777Z","iopub.status.idle":"2021-10-02T17:14:11.649446Z","shell.execute_reply.started":"2021-10-02T17:14:11.632744Z","shell.execute_reply":"2021-10-02T17:14:11.648543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.describe()","metadata":{"execution":{"iopub.status.busy":"2021-10-02T17:14:12.46119Z","iopub.execute_input":"2021-10-02T17:14:12.461585Z","iopub.status.idle":"2021-10-02T17:14:12.484151Z","shell.execute_reply.started":"2021-10-02T17:14:12.461555Z","shell.execute_reply":"2021-10-02T17:14:12.483436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.isna(train_df).sum()/len(train_df)","metadata":{"execution":{"iopub.status.busy":"2021-10-02T17:14:13.070692Z","iopub.execute_input":"2021-10-02T17:14:13.071227Z","iopub.status.idle":"2021-10-02T17:14:13.082315Z","shell.execute_reply.started":"2021-10-02T17:14:13.071193Z","shell.execute_reply":"2021-10-02T17:14:13.081308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"positive_class=train_df['target'].sum()/len(train_df)\nprint(f'the percentage of ones in the dataset is {np.round(positive_class,2)}')","metadata":{"execution":{"iopub.status.busy":"2021-10-02T17:14:14.107933Z","iopub.execute_input":"2021-10-02T17:14:14.108311Z","iopub.status.idle":"2021-10-02T17:14:14.114128Z","shell.execute_reply.started":"2021-10-02T17:14:14.10828Z","shell.execute_reply":"2021-10-02T17:14:14.11325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2><b>Simple Methods</b></h2>\n\n<h4 style='text-align: justify;'>\nFirst I did submit the sample results. It achieved a 57% accuracy score. Afterwards, I used generated output files from each of the keyword and location columns. Hence, we can sense what score can be reached using easy methods and whatmore advanced ones can do.\n</h4>","metadata":{}},{"cell_type":"code","source":"keyword_df=pd.DataFrame()\ni=0\nfor keyword,group_df in train_df[['keyword','target']].fillna('unk').groupby('keyword'):\n    positive_Kkeyword=group_df['target'].sum()/len(group_df)\n    keyword_df.loc[i,['keyword']]=keyword\n    keyword_df.loc[i,['count']]=len(group_df)\n    keyword_df.loc[i,['% positive']]=positive_Kkeyword\n    i+=1\n#keyword_df.to_csv('keyword_df.csv')","metadata":{"execution":{"iopub.status.busy":"2021-10-02T17:14:15.081682Z","iopub.execute_input":"2021-10-02T17:14:15.08233Z","iopub.status.idle":"2021-10-02T17:14:15.853425Z","shell.execute_reply.started":"2021-10-02T17:14:15.082295Z","shell.execute_reply":"2021-10-02T17:14:15.852653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"location_df=pd.DataFrame()\ni=0\nfor keyword,group_df in train_df[['location','target']].fillna('unk').groupby('location'):\n    positive_Kkeyword=group_df['target'].sum()/len(group_df)\n    location_df.loc[i,['location']]=keyword\n    location_df.loc[i,['count']]=len(group_df)\n    location_df.loc[i,['% positive']]=positive_Kkeyword\n    i+=1\n#location_df.to_csv('location_df.csv')","metadata":{"execution":{"iopub.status.busy":"2021-09-12T17:28:37.380727Z","iopub.execute_input":"2021-09-12T17:28:37.381152Z","iopub.status.idle":"2021-09-12T17:28:49.763947Z","shell.execute_reply.started":"2021-09-12T17:28:37.381118Z","shell.execute_reply":"2021-09-12T17:28:49.763224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4>The idea is to group each row with the same column value (for keyword_df the keyword column while for the location_df the location column). Afterwards, some useful information is added, the most important one (the one that is used in the model) is the % positive which is basically the mean of the target for each group.</h4>","metadata":{}},{"cell_type":"code","source":"keyword_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-12T17:28:49.765147Z","iopub.execute_input":"2021-09-12T17:28:49.76544Z","iopub.status.idle":"2021-09-12T17:28:49.77751Z","shell.execute_reply.started":"2021-09-12T17:28:49.76541Z","shell.execute_reply":"2021-09-12T17:28:49.776365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(keyword_df[keyword_df['keyword']==test_df.iloc[0]['keyword']]['% positive'].values[0])\nnew_submission=pd.DataFrame()\nnew_submission['id']=test_df['id']\nnew_submission['target']=test_df.apply(lambda row: int(keyword_df[keyword_df.keyword==row['keyword']]['% positive'].values[0]>=0.5),axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-09-12T17:28:49.949971Z","iopub.execute_input":"2021-09-12T17:28:49.950478Z","iopub.status.idle":"2021-09-12T17:28:51.786422Z","shell.execute_reply.started":"2021-09-12T17:28:49.95044Z","shell.execute_reply":"2021-09-12T17:28:51.78566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#new_submission.rename({0:'target'},axis=1)\nnew_submission.to_csv('to_submit.csv')","metadata":{"execution":{"iopub.status.busy":"2021-09-12T17:28:51.787537Z","iopub.execute_input":"2021-09-12T17:28:51.787993Z","iopub.status.idle":"2021-09-12T17:28:51.805557Z","shell.execute_reply.started":"2021-09-12T17:28:51.787943Z","shell.execute_reply":"2021-09-12T17:28:51.804217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_submission","metadata":{"execution":{"iopub.status.busy":"2021-09-12T17:28:51.807017Z","iopub.execute_input":"2021-09-12T17:28:51.807378Z","iopub.status.idle":"2021-09-12T17:28:51.822717Z","shell.execute_reply.started":"2021-09-12T17:28:51.807345Z","shell.execute_reply":"2021-09-12T17:28:51.82168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4>\nThe keyword generated output file got 72% score while the location file got a lower score. The location output file generated a lower score than the keyword. This is expected because the location column contains a lot of null values. <br/>\nNotice the difference between the sample submission (57%) and the result only using the keyword (72%). This shows the value of the keyword column for this task. It is expected because the keyword is a summary of the tweet using one word.\n</h4>","metadata":{}},{"cell_type":"markdown","source":"<h2><b>Visualizations</b></h2>","metadata":{}},{"cell_type":"code","source":"train_ls=list(train_df['text'].str.split(' ').to_numpy())","metadata":{"execution":{"iopub.status.busy":"2021-10-02T17:14:22.814743Z","iopub.execute_input":"2021-10-02T17:14:22.815252Z","iopub.status.idle":"2021-10-02T17:14:22.840264Z","shell.execute_reply.started":"2021-10-02T17:14:22.81522Z","shell.execute_reply":"2021-10-02T17:14:22.839273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Tokenizer=keras.preprocessing.text.Tokenizer(oov_token=0)\nTokenizer.fit_on_texts(train_ls)","metadata":{"execution":{"iopub.status.busy":"2021-10-02T17:14:28.288812Z","iopub.execute_input":"2021-10-02T17:14:28.289372Z","iopub.status.idle":"2021-10-02T17:14:28.44011Z","shell.execute_reply.started":"2021-10-02T17:14:28.289322Z","shell.execute_reply":"2021-10-02T17:14:28.43925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"i=0\nn_words=len(Tokenizer.word_counts)\nfor word,count in Counter(Tokenizer.word_counts).most_common():\n    print(f'{word}: {count}')\n    i+=1\n    if count<100:\n        break\n\nprint(f'{i} word are the most common from {n_words} which is {i/n_words*100}%')","metadata":{"execution":{"iopub.status.busy":"2021-10-02T17:14:28.528232Z","iopub.execute_input":"2021-10-02T17:14:28.528793Z","iopub.status.idle":"2021-10-02T17:14:28.563649Z","shell.execute_reply.started":"2021-10-02T17:14:28.528746Z","shell.execute_reply":"2021-10-02T17:14:28.562576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(x='target',data=train_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_characters=np.max(train_df['text'].apply(lambda x: len(x)))\nprint(f'the max number of characters in a tweet is {max_characters}\\nThus, the number of word < {max_characters}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type(Counter(Tokenizer.word_counts).most_common())","metadata":{"execution":{"iopub.status.busy":"2021-10-02T17:14:28.733882Z","iopub.execute_input":"2021-10-02T17:14:28.734251Z","iopub.status.idle":"2021-10-02T17:14:28.758147Z","shell.execute_reply.started":"2021-10-02T17:14:28.734221Z","shell.execute_reply":"2021-10-02T17:14:28.756846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top_100=Counter(Tokenizer.word_counts).most_common()[:100]\n\nx_top_100=[x for x,y in top_100]\ny_top_100=[y for x,y in top_100]\nax=sns.barplot(x=y_top_100,y=x_top_100)\nax.figure.set_size_inches(15,20)","metadata":{"execution":{"iopub.status.busy":"2021-10-02T17:14:29.199667Z","iopub.execute_input":"2021-10-02T17:14:29.200025Z","iopub.status.idle":"2021-10-02T17:14:30.909717Z","shell.execute_reply.started":"2021-10-02T17:14:29.199994Z","shell.execute_reply":"2021-10-02T17:14:30.908653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4 style='text-align: justify;'>\nLooking at the top 100 words makes us realized the most used words like 'a' do not give a lot of information if a disaster did occur or not. Actually, only emergency and disaster are word that indicate the possibility of a disaster occuring. Hence, most of the words are stop words that do not give much information about the tweet itself. These words will be removed.\n</h4>","metadata":{}},{"cell_type":"code","source":"bottom_100=Counter(Tokenizer.word_counts).most_common()[-100:]\n\nx_bottom_100=[x for x,y in bottom_100]\ny_bottom_100=[y for x,y in bottom_100]\n\n#Horizontal barplot\nax=sns.barplot(x=y_bottom_100,y=x_bottom_100)\nax.figure.set_size_inches(15,20)","metadata":{"execution":{"iopub.status.busy":"2021-10-02T17:14:30.91122Z","iopub.execute_input":"2021-10-02T17:14:30.911521Z","iopub.status.idle":"2021-10-02T17:14:33.312379Z","shell.execute_reply.started":"2021-10-02T17:14:30.911495Z","shell.execute_reply":"2021-10-02T17:14:33.31112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4>\nThe bottom 100 words are all links. I do not think that the presence of a link means a disater happened. I do not expect that a disaster happened if someone tweeted 'check this link http:...'. If there was a link or a radio source that only give information about disasters. Hence, the links will be removed too.\n</h4>","metadata":{}},{"cell_type":"markdown","source":"<h2><b>Process Tweets</b></h2>\n\n<h4>\nThe tweets are processed using the nltk library.\n</h4>","metadata":{}},{"cell_type":"code","source":"from nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.tokenize import RegexpTokenizer\nimport string\nimport re","metadata":{"execution":{"iopub.status.busy":"2021-10-02T17:17:41.895471Z","iopub.execute_input":"2021-10-02T17:17:41.895837Z","iopub.status.idle":"2021-10-02T17:17:41.900647Z","shell.execute_reply.started":"2021-10-02T17:17:41.895808Z","shell.execute_reply":"2021-10-02T17:17:41.899505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stop=stopwords.words('english')\n\n# for i in stop:\n#     print(i)","metadata":{"execution":{"iopub.status.busy":"2021-10-02T17:14:33.322318Z","iopub.execute_input":"2021-10-02T17:14:33.322651Z","iopub.status.idle":"2021-10-02T17:14:33.341709Z","shell.execute_reply.started":"2021-10-02T17:14:33.322619Z","shell.execute_reply":"2021-10-02T17:14:33.340756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_df=pd.DataFrame()\n\ntrain_df['trainable']=1\ntest_df['trainable']=0\n\ntotal_df=pd.concat((train_df,test_df))\ntotal_df","metadata":{"execution":{"iopub.status.busy":"2021-10-02T17:14:33.343293Z","iopub.execute_input":"2021-10-02T17:14:33.343735Z","iopub.status.idle":"2021-10-02T17:14:33.368627Z","shell.execute_reply.started":"2021-10-02T17:14:33.343668Z","shell.execute_reply":"2021-10-02T17:14:33.367563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ls_tmp=[]\nstemmer= PorterStemmer()","metadata":{"execution":{"iopub.status.busy":"2021-10-02T17:14:33.815327Z","iopub.execute_input":"2021-10-02T17:14:33.81569Z","iopub.status.idle":"2021-10-02T17:14:33.820073Z","shell.execute_reply.started":"2021-10-02T17:14:33.815659Z","shell.execute_reply":"2021-10-02T17:14:33.81859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_stem(word:str,stemmer):\n    '''\n    Get the stem of the word\n    '''\n    return stemmer.stem(word)","metadata":{"execution":{"iopub.status.busy":"2021-10-02T17:14:34.187645Z","iopub.execute_input":"2021-10-02T17:14:34.18804Z","iopub.status.idle":"2021-10-02T17:14:34.193187Z","shell.execute_reply.started":"2021-10-02T17:14:34.188008Z","shell.execute_reply":"2021-10-02T17:14:34.192036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_sentence(text,stemmer):\n    new_sentence=[]\n    \n    text = text.replace(r\"won't \", \"will not \")\n    text = text.replace(r\"can't \", \"can not \")\n    text = text.replace(r\"ain't \", \"am not \")\n    \n    text = text.replace(r\"n't \", \" not \")\n    text = text.replace(r\"'re \", \" are \")\n    text = text.replace(r\"'s \", \" is \")\n    text = text.replace(r\"'d \", \" would \")\n    text = text.replace(r\"'ll \", \" will \")\n    text = text.replace(r\"'t \", \" not \")\n    text = text.replace(r\"'ve \", \" have \")\n    text = text.replace(r\"'m \", \" am \")\n    \n    text = text.translate(str.maketrans('','',string.punctuation))\n    \n    text=re.sub(r'[^a-zA-Z0-9 ]', '', text)\n    #print(text)\n    sentence=text.split(' ')\n    \n    sentence=list(filter(lambda a: a != '', sentence))\n    \n    for word in sentence:\n        #word=stemmer.stem(word)\n        word_lower=word.lower()\n        if (word_lower not in stop) and ('http' not in word_lower):# and ():\n            new_sentence.append(word_lower)\n    \n    return ' '.join(new_sentence)\n\npreprocess_sentence(\"Hey I'm Yann, ° Ñ  how're you and how's it going ? That's interesting: I'd love to hear more about it. http:idk.com\",stemmer)","metadata":{"execution":{"iopub.status.busy":"2021-10-02T17:25:56.952324Z","iopub.execute_input":"2021-10-02T17:25:56.952694Z","iopub.status.idle":"2021-10-02T17:25:56.965888Z","shell.execute_reply.started":"2021-10-02T17:25:56.952665Z","shell.execute_reply":"2021-10-02T17:25:56.965229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_df['preprocessed_text']=total_df['text'].apply(lambda text: preprocess_sentence(text,stemmer))\ntotal_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-02T17:25:57.332672Z","iopub.execute_input":"2021-10-02T17:25:57.333025Z","iopub.status.idle":"2021-10-02T17:25:57.931243Z","shell.execute_reply.started":"2021-10-02T17:25:57.332994Z","shell.execute_reply":"2021-10-02T17:25:57.930119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_df.to_csv('preprocessed_text.csv')","metadata":{"execution":{"iopub.status.busy":"2021-10-02T17:25:57.932847Z","iopub.execute_input":"2021-10-02T17:25:57.933176Z","iopub.status.idle":"2021-10-02T17:25:58.066791Z","shell.execute_reply.started":"2021-10-02T17:25:57.933145Z","shell.execute_reply":"2021-10-02T17:25:58.065764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1>Conclusion</h1>\n\n<h4 style='text-align: justify;'>\nThis notebook started by generating outputs using simple methods from the non-tweet features. The keyword output got 72% accuracy. Afterwards, some plots for visualization were shown. The plot showed the effect of stop words and links. Finally, the text was preprocessed to be used with BERT.\n</h4>\n\n<h4>\n    Checkout my BERT notebook <a href=''>Here</a>\n</h4>","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}