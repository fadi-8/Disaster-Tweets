{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":" <h1><b><u>Disaster Tweets Classification </u></b></h1>","metadata":{"id":"1rTC3cFiLSbv"}},{"cell_type":"markdown","source":"Disaster Tweet Classification is a competition from kaggle. I already have a notebook that preprocessed the data. In this notebook I will use the preprocessed data to make predictions using a BERT model.\n\nBERT is a neural network classifier based on attention. It can be used for a lot of tasks, like sentence prediction, named entity recognition,...; among the tasks BERT can be used for is sentiment analysis. Sentiment analysis,normally, predicts the sentiment (positive or negative, for example) of the input text. Similarly, the disaster tweet task can be formulated where 1 means a disaster and 0 means a non-disaster tweet. Note, the BERT model is used in sentiment analysis by adding an ouput layer after BERT (or another neural network that contains an output).\n\nTasks that have to deal with text are usually variable length. On the other hand, many machine learning models expect a fixed length input. Thus, We can use the BERT model to map the input text to a fixed length space. Afterwards, the fixed length output can be feeded to any machine learning model.\n\nThe data can be found <a href=https://www.kaggle.com/c/nlp-getting-started/data>Here</a>\n\nI have a kaggle notebook to preprocess the data: <a href='https://www.kaggle.com/fmakarem/disaster-tweets'>Preprocess Notebook</a>\n\n<b>A version of this notebook containing the cells outputs</b>:<a href=\"https://github.com/fadi-8/Disaster-Tweets\"> Disaster Tweets ML with outputs </a>","metadata":{"id":"gLpK0ZRYKst_"}},{"cell_type":"markdown","source":"<h4><b>1-Imports and BERT Model</b></h4>","metadata":{"id":"QN12SSAYe764"}},{"cell_type":"code","source":"!pip install tensorflow-text","metadata":{"id":"i6TgQmr6bvPh","outputId":"2798cae5-f7a4-4941-f0ce-61e9b446bb93","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.callbacks import EarlyStopping,LearningRateScheduler\nfrom tensorflow.keras.utils import plot_model\nimport tensorflow_hub as hub\nimport tensorflow_text\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import KFold,StratifiedKFold,train_test_split\nfrom sklearn.manifold import TSNE\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import cross_validate,GridSearchCV\nfrom sklearn.base import clone\nimport pandas as pd\nimport os\nfrom sys import getsizeof","metadata":{"id":"6ITgOxg8IESJ","execution":{"iopub.status.busy":"2021-11-09T16:09:20.677297Z","iopub.execute_input":"2021-11-09T16:09:20.677568Z","iopub.status.idle":"2021-11-09T16:09:21.942732Z","shell.execute_reply.started":"2021-11-09T16:09:20.677541Z","shell.execute_reply":"2021-11-09T16:09:21.941744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_splits=5\nrandom_state=27","metadata":{"id":"BqXtrRPrWd8U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To attain an useful model, a lot of training time is needed. A method to decrease the time and get even better results is by transfer learning. Transfer learning is the concept of using a trained model (with good performance) on a specific task and use its knowledge (learned weights) in another task. In this way, instead of starting with a random guess of the parameters that are unrelated to the task, with start with parameters containing information somehow related to our own task.\n\nTensorflow hub is a platform used to get trained models that are ready to use. The BERT model used is from tensorflow hub. Go to tensorfow hub, search for the model you need, check the guidelines.\n\nTensorflow hub is used below to get the BERT model.","metadata":{"id":"FYu-1wZ_YrKM"}},{"cell_type":"code","source":"#text_input = tf.keras.layers.Input(shape=(), dtype=tf.string)\n\npreprocessor = hub.KerasLayer(\n    # \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\")\n    \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\")\n\n#encoder_inputs = preprocessor(text_input)\n\nencoder = hub.KerasLayer(\n    # \"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/4\",\n    \"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/2\",\n    trainable=True)\n\n#outputs = encoder(encoder_inputs)\n#pooled_output = outputs[\"pooled_output\"]      # [batch_size, 768].\n#sequence_output = outputs[\"sequence_output\"]  # [batch_size, seq_length, 768].","metadata":{"id":"ywcIYEE_YiFA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Create the BERT model","metadata":{"id":"aYIRdHR0Zdmn"}},{"cell_type":"code","source":"text_inputs=keras.layers.Input(shape=(),dtype=tf.string)\npreprocessed_text=preprocessor(text_inputs)\noutputs=encoder(preprocessed_text)\n\nBERT_model=keras.Model(text_inputs,outputs['pooled_output'],name='BERT_Model')","metadata":{"id":"-0zv40V3Wuuc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4><b>2-Helper Functions</b></h4>\n\nThe functions are used in the proceeding parts to fine-tune and train the models.","metadata":{"id":"FTPnJvEKmBn3"}},{"cell_type":"code","source":"def fine_tune_model(rate=0.2):\n  #fine-tuning model used\n  #rate=0.2\n\n  text_inputs=keras.layers.Input(shape=(),dtype=tf.string,name='Text_Input')\n  keyword_inputs=keras.layers.Input(shape=(1,),name='Keyword_Stats')\n\n  # preprocessed_text=preprocessor(text_inputs)\n  # outputs=encoder(preprocessed_text)\n\n  BERT_output=BERT_model(text_inputs)\n\n  #x=keras.layers.Dropout(rate=rate,name='Dropout_on_BERT')(BERT_output)#outputs['pooled_output'])\n\n  #x=keras.layers.Conv1D(30,3)(tf.expand_dims(x,-1))\n\n  #x=keras.layers.Flatten()(x)\n\n  x=keras.layers.Concatenate(name='Concat_BERT_keywordStats')([BERT_output,keyword_inputs])#x\n\n  #x=keras.layers.Dense(20,activation='relu')(x)\n  x=keras.layers.Dropout(rate=rate,name='Dropout_1')(x)\n\n  y=keras.layers.Dense(1,activation='sigmoid',name='output')(x)\n\n  model=keras.Model([text_inputs,keyword_inputs],y,name='Pre-train_Model')\n\n  model.compile(optimizer=keras.optimizers.Adam(1e-5,clipvalue=2),loss=keras.losses.binary_crossentropy,metrics=['accuracy'])\n  \n  return model","metadata":{"id":"_s7M0bZF4YAl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_keyword_mean(train_df,val_df,column='keyword',target='target'):\n    aggregate_column=column+'_mean'\n    train_df[aggregate_column]=train_df.groupby(column)[target].transform('mean')\n    \n    val_df = val_df.merge(\n                train_df[[column, aggregate_column]].drop_duplicates(),\n                on=column,\n                how=\"left\",\n            )\n    return train_df,val_df","metadata":{"id":"doLi3dBkmBJv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def valuate_model(df,model_creation_function,target_column='target',use_keyword=True,compile_dic={},**kwargs):\n  '''\n  df: it should not contain the test data.\n  model_creation_function: function used to create the mode. It should contain BERT_Model\n  '''\n  #kfold=KFold(n_splits=n_splits,shuffle=True,random_state=random_state)\n  kfold=StratifiedKFold(n_splits=n_splits,shuffle=True,random_state=random_state)\n  results={'Total':0}\n  i=0\n  \n  df['prediction']=np.nan\n  #iterate through the folds\n  for train_index,val_index in kfold.split(df,df[target_column]):\n      \n    #PREPARE THE INPUTS\n\n    train_df=df.iloc[train_index]\n    val_df=df.iloc[val_index]\n    \n    # if use_keyword:\n    #     train_df,val_df=get_keyword_mean(train_df,val_df)\n    #     #regex+='|^keyword_'\n    \n    X_train_keyword=train_df.filter(regex='keyword_',axis=1).astype(float).values\n    X_val_keyword=val_df.filter(regex='keyword_',axis=1).astype(float).values\n    \n    X_train_text=train_df['preprocessed_text'].astype(str).values\n    y_train=train_df[target_column].values\n    \n    X_val_text=val_df['preprocessed_text'].astype(str).values\n    y_val=val_df[target_column].values\n    \n    #print(X_train)\n    # X_train=X_train.copy().values\n    # y_train=y_train.copy().values\n    \n    # X_val=X_val.copy().values\n    # y_val=y_val.copy().values\n\n    model_copy=model_creation_function()\n    #BERT_initial_weights=BERT_model.get_weights()\n    BERT_model.trainable=False\n    BERT_model.compile(**compile_dic)\n    model_copy.compile(**compile_dic)\n\n    if i==0:\n      model_copy.summary()\n\n    model_copy.fit([X_train_text,X_train_keyword],\n                   y_train,\n                   validation_data=([X_val_text,X_val_keyword],y_val),\n                   **kwargs)\n    \n    #make sure the weights of the BERT model did not change\n    #np.testing.assert_allclose(BERT_model.get_weights(),BERT_initial_weights)\n\n    predicted=model_copy.predict([X_val_text,X_val_keyword])\n    \n    df.loc[df.index.isin(df.iloc[val_index].index),['prediction']]=predicted\n\n    print(df.columns)\n    print(df.head)\n\n    predicted=(predicted>0.5).astype(int)\n    #df.iloc[val_index,['prediction_binary']]=predicted\n\n    #COMPUTE THE CONFUSION MATRIX\n    metric=confusion_matrix(y_val,predicted)\n    \n    tn, fp, fn, tp = metric.ravel()\n    \n    results_matrix=metric\n    results['Batch '+str(i)]=results_matrix \n    results['Total']+=np.array(results_matrix)\n    i+=1\n    \n    print(f'{i}th run:\\naccuracy: {(tp+tn)/(tn+fp+fn+tp)}\\nprecision:{tp/(tp+fp)}\\nrecall:{tp/(tp+fn)}\\n')\n  \n  df['prediction_binary']=(df['prediction']>0.5).astype(int)\n\n  tn, fp, fn, tp = results['Total'].ravel()\n  \n  print(f'Total:\\naccuracy: {(tp+tn)/(tn+fp+fn+tp)}\\nprecision:{tp/(tp+fp)}\\nrecall:{tp/(tp+fn)}\\n')\n  \n  return results,df","metadata":{"id":"ntIhxTCHVegm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def check_misclassifications(df,target_column='target',predicted_column='prediction_binary'):\n  misclassifications=df[df[target_column]!=df[predicted_column]]\n  return misclassifications","metadata":{"id":"wrSK9GtQOqme"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"NOTE: a small learning rate and clipvalue are used to prevent the model from forgetting its original weight values. In other words, to prevent it from throwing to away the information it already learned.","metadata":{"id":"ihLxrXODdNit"}},{"cell_type":"markdown","source":"<h4><b>3-Get The Data and Fine-Tune BERT Model</b></h4>\n\nIn this part the data will be fetched onto DataFrames. Furthermore, the effect of fine-tuning the BERT model is demostrated using TSNE plots.\n\nThe data is already preprocessed. Check the preprocessing notebook <a href=\"https://www.kaggle.com/fmakarem/disaster-tweets\">Here<a/>","metadata":{"id":"SYAY_FCMgJ6m"}},{"cell_type":"code","source":"df=pd.read_csv('../input/preprocessed-data/preprocessed_data.csv')\n\ndf['keyword']=df['keyword'].fillna('unk')\n\ntrain_df=df[df['trainable']==1]\ntest_df=df[~(df['trainable']==1)]\n\ntrain_df,test_df=get_keyword_mean(train_df,test_df)\n\ntrain_df['preprocessed_text']=train_df['preprocessed_text'].fillna('Great').astype(str)\ntest_df['preprocessed_text']=test_df['preprocessed_text'].fillna('Great').astype(str)\n\n# train_df,fine_tune_df=train_test_split(total_train_df[['preprocessed_text','keyword_mean','target']],\n#                                        test_size=0.2,\n#                                        stratify=total_train_df['target'],\n#                                        random_state=random_state)","metadata":{"id":"w4JerS8elPrk","outputId":"55674ddd-13ff-4afd-d5f4-3088c58cd4bd","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_df[train_df['target']==1])/(len(train_df))","metadata":{"id":"Ptr37oTLAppC","outputId":"0518b4e0-b343-4fb9-c077-e8222ca09227"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_df),len(test_df)","metadata":{"id":"x4zPr9FJqzFw","outputId":"1ec25775-7992-49b5-9b1b-f667ef6ad2e3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b>Fine-Tunning</b>\n\nThere are 2 stages for BERT models to be used. First, the BERT models are pre-trained using tons of data. Pre-training can be predicting the next word in the sentence given the previous words. Hence, this stage allows the model to learn useful representations for the languague in general. The second stage is fine-tuning the BERT model. After having an initial set of learned parameters related to the task, the next step is learn the parameters that are best for the task tackled. Thus, learning parameters specific to the task instead of parameters related to the languague in general. This allows for better and faster results than starting by random guesses of the parameters.\n\nThe model downloaded from tensorflow hub is already pre-trained. We need to fine-tune it. But, let us show the effect of fine-tuning using TSNE plots.\n","metadata":{"id":"8R4fA_0RjgP-"}},{"cell_type":"markdown","source":"<b>TSNE Plots</b>\n\nTSNE stands for t-distributed stochastic neighbor embedding. It is an unsupervised manifold method used represent high dimensional data into lower dimentional data. Hence, dimensionality reduction. Although, TSNE is used to visualize the data.\n\nTSNE is used to reduce the data dimentionality to just 2D (2 components). Afterwards, the data is plotted to visualize the representation outputed by the BERT model before and after fine-tuning.","metadata":{"id":"6RWRpt_d0U6H"}},{"cell_type":"markdown","source":"<b>TSNE Plot Before Fine-Tuning</b>","metadata":{"id":"8ElSD6H9akSv"}},{"cell_type":"code","source":"X = BERT_model.predict(train_df['preprocessed_text'].values)\nX_embedded = TSNE(n_components=2,random_state=random_state).fit_transform(X)\n\ny=train_df['target'].values\n\ntsne_df = pd.DataFrame()\ntsne_df[\"y\"] = y\ntsne_df[\"comp-1\"] = X_embedded[:,0]\ntsne_df[\"comp-2\"] = X_embedded[:,1]\n\nsns.scatterplot(x=\"comp-1\", y=\"comp-2\", hue=tsne_df.y.tolist(),\n                palette=sns.color_palette(\"hls\", 2),\n                data=tsne_df).set(title=\"BERT embeddings for disaster and non disaster tweets\") ","metadata":{"id":"L1LH_Q3MXhSf","outputId":"bee2f6e7-bbcc-419e-e97f-4fd002302cd7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The plot before fine-tuning shows that the classes are difficult to separate. Even in the locations that are dominated by a certain class, there are a lot of data from the opposing class. Thus, making the task tricky for any model. Actually, random forest and Support Vector Machine (SVM) were tried using BERT representation before fine-tuning. The best result was for SVM around 79% while a fine-tuned model reached around 81.4%.","metadata":{"id":"bbWbu2PLbJvs"}},{"cell_type":"markdown","source":"<b>Fine-Tune The BERT Model</b>","metadata":{"id":"VBjqsgSb2zwL"}},{"cell_type":"code","source":"model=fine_tune_model(rate=0.2)\n\nmodel.summary()\n\nhistory=model.fit([train_df['preprocessed_text'].values,\n          train_df['keyword_mean'].values],\n          train_df['target'].values,\n          batch_size=64,\n          epochs=30)\n","metadata":{"id":"KnwPraVblhDg","outputId":"72f76c4f-b643-455a-c3a6-c1d9c22f351b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history['accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train'], loc='upper left')\nplt.show()","metadata":{"id":"9XQKMgtaHuVS","outputId":"531364e5-4e6c-4433-e8db-9991c5528848"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#@title plot_model\nplot_model(model, to_file='multiple_inputs.png')","metadata":{"cellView":"form","id":"dWRbGXRcppv5","outputId":"7d3d1bc6-c6ed-499b-9149-0e9990d0dfdc","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"NOTE: the perforance of the model is of little importance since we only aim to fine-tune it to learn a good representation of the data to be used for other models.","metadata":{"id":"_L2f8oiKcv4q"}},{"cell_type":"markdown","source":"<b>TSNE Plot After Fine-Tuning</b>","metadata":{"id":"YF4teo1ra1NB"}},{"cell_type":"code","source":"X = BERT_model.predict(train_df['preprocessed_text'].values)\nX_embedded = TSNE(n_components=2,random_state=random_state).fit_transform(X)\n\ny=train_df['target'].values\n\ntsne_df = pd.DataFrame()\ntsne_df[\"y\"] = y\ntsne_df[\"comp-1\"] = X_embedded[:,0]\ntsne_df[\"comp-2\"] = X_embedded[:,1]\n\nsns.scatterplot(x=\"comp-1\", y=\"comp-2\", hue=tsne_df.y.tolist(),\n                palette=sns.color_palette(\"hls\", 2),\n                data=tsne_df).set(title=\"BERT embeddings for disaster and non disaster tweets after fine tuning\") ","metadata":{"id":"RnX2XeQHa3Jy","outputId":"607e62ca-31f3-44a2-d650-5db41c2ee6a8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The TSNE plot after fine-tuning shows that the classes are now easier to separate. Hence, the BERT model learned a better representation of the classes in the data. Thus, making the prediction task easier for models.","metadata":{"id":"L-Lg72clcOxX"}},{"cell_type":"markdown","source":"<h4><b>4-Teting The Models</b></h4>\n\nIn this part different models are tested using K-Fold cross validation with K given at the beginning as 5. Note the BERT model parameters are freezed. Only the new added layers are trained.","metadata":{"id":"HC-mGrPk5puD"}},{"cell_type":"code","source":"compile_dic={'optimizer':keras.optimizers.Adam(1e-5,clipvalue=2),\n             'loss':keras.losses.binary_crossentropy,\n             'metrics':['accuracy']}\nbatch_size=64\nepochs=30\ncallbacks=[EarlyStopping(monitor='val_accuracy',min_delta=0.02,patience=3,mode='max')]\nmodel1_results,model1_df=valuate_model(train_df,\n                                       fine_tune_model,\n                                       target_column='target',\n                                       use_keyword=True,\n                                       compile_dic=compile_dic,\n                                       batch_size=batch_size,\n                                       callbacks=callbacks,\n                                       epochs=epochs)","metadata":{"id":"lfpiHuTKX0Zq","outputId":"0b44f63f-c236-40a6-8abc-453df5c80001"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This model gave a score above 81% on the competition with 30 epochs using the whole training set. Yet we can see that the tests using 15 epochs after finetuning the BERT_model did give good results on some validation sets and bad results on another sets. Thus, either a lot of outliers are present in the validation sets that the model did badly or there are 2 different distributions in the test set.<br/>\nThe data in these sets should be checked.","metadata":{"id":"zMuT83bLPgYR"}},{"cell_type":"code","source":"#@title\nkfold=StratifiedKFold(n_splits=n_splits,shuffle=True,random_state=random_state)\n\nwanted_indexes=[]\ni=1\n#iterate through the folds\nfor train_index,val_index in kfold.split(train_df,train_df['target']):\n  if (i==5) or (i==4):\n    wanted_indexes+=val_index.tolist()\n  \n  i+=1\n\ntrain_df.iloc[wanted_indexes].to_csv('outlier.csv')","metadata":{"cellView":"form","id":"3H8rFTUKSRTz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b>Model 1</b>","metadata":{"id":"Gr7MPVjTXmtH"}},{"cell_type":"code","source":"def model1():\n  rate=0.2\n\n  text_inputs=keras.layers.Input(shape=(),dtype=tf.string,name='Text_Input')\n  keyword_inputs=keras.layers.Input(shape=(1,),name='Keyword_Stats')\n\n  # preprocessed_text=preprocessor(text_inputs)\n  # outputs=encoder(preprocessed_text)\n\n  BERT_output=BERT_model(text_inputs)\n\n  x=keras.layers.Dropout(rate=rate,name='Dropout_on_BERT_1')(BERT_output)#outputs['pooled_output'])\n\n  #x=keras.layers.Conv1D(30,3)(tf.expand_dims(x,-1))\n\n  #x=keras.layers.Flatten()(x)\n\n  x=keras.layers.Concatenate(name='Concat_BERT_keywordStats_1')([x,keyword_inputs])\n\n  x=keras.layers.Dense(20,activation='relu',name='Dense_layer_1')(x)\n  x=keras.layers.Dropout(rate=rate,name='Dropout_1_1')(x)\n\n  y=keras.layers.Dense(1,activation='sigmoid',name='output_1',kernel_regularizer=keras.regularizers.L2())(x)\n\n  model=keras.Model([text_inputs,keyword_inputs],y,name='Model_1')\n\n  #model.compile()\n  \n  return model","metadata":{"id":"kLk4XuAEPgKa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"compile_dic={'optimizer':keras.optimizers.Adam(1e-5,clipvalue=2),\n             'loss':keras.losses.binary_crossentropy,\n             'metrics':['accuracy']}\nbatch_size=64\nepochs=25\ncallbacks=[EarlyStopping(monitor='val_accuracy',min_delta=0.02,patience=3,mode='max')]\nmodel1_results,model1_df=valuate_model(train_df,\n                                       model1,\n                                       target_column='target',\n                                       use_keyword=True,\n                                       compile_dic=compile_dic,\n                                       batch_size=batch_size,\n                                       epochs=epochs,\n                                       callbacks=callbacks)","metadata":{"id":"Wr6wf2BUQiC4","outputId":"2bbb7044-66d6-4bac-b94c-8779cb318659","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Model 1 actually got 80.171% score on the test set.","metadata":{"id":"U8IbtXZrW2FG"}},{"cell_type":"code","source":"test_df['preprocessed_text']=test_df['preprocessed_text'].fillna('great')\ntest_df['predictions']=(model.predict([test_df['preprocessed_text'].values,test_df['keyword_mean'].values])>0.5).astype(int)\ntest_df[['id','predictions']].set_index('id').rename(columns={'predictions':'target'}).to_csv('model1_submission.csv')","metadata":{"id":"KEuM77nbuK9_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b>Model 2</b>","metadata":{"id":"q4RfONo_XhFR"}},{"cell_type":"code","source":"def model2():\n  rate=0.4\n\n  text_inputs=keras.layers.Input(shape=(),dtype=tf.string,name='Text_Input')\n  keyword_inputs=keras.layers.Input(shape=(1,),name='Keyword_Stats')\n\n  # preprocessed_text=preprocessor(text_inputs)\n  # outputs=encoder(preprocessed_text)\n\n  BERT_output=BERT_model(text_inputs)\n\n  x=keras.layers.Dropout(rate=rate,name='Dropout_on_BERT_2')(BERT_output)#outputs['pooled_output'])\n\n  dropout_keyword=keras.layers.Dropout(rate=0.1,name='Dropout_on_keyword')(keyword_inputs)#outputs['pooled_output'])\n  \n  #x=keras.layers.Conv1D(30,3)(tf.expand_dims(x,-1))\n\n  #x=keras.layers.Flatten()(x)\n  \n  #x=keras.layers.Concatenate(name='Concat_BERT_keywordStats_2')([x,keyword_inputs])\n  x=keras.layers.Concatenate(name='Concat_BERT_keywordStats_2')([x,dropout_keyword])\n\n  x=tf.expand_dims(x,axis=-1)\n  x=keras.layers.Conv1D(100,5,activation='relu',name='Conv_layer_1')(x)\n  #x=keras.layers.Flatten()(x)\n  x=keras.layers.GlobalMaxPooling1D()(x)\n  x=keras.layers.Dropout(rate=rate,name='Dropout_1_2')(x)\n\n  x=keras.layers.Dense(20,activation='relu',name='hidden_layer_1')(x)\n\n  y=keras.layers.Dense(1,activation='sigmoid',name='output_2',kernel_regularizer=keras.regularizers.L2())(x)\n\n  model=keras.Model([text_inputs,keyword_inputs],y,name='Model_2')\n\n  #model.compile()\n  \n  return model","metadata":{"id":"JkCwlyjIRC6t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"compile_dic={'optimizer':keras.optimizers.Adam(1e-3,clipvalue=2),\n             'loss':keras.losses.binary_crossentropy,\n             'metrics':['accuracy']}\nbatch_size=64\nepochs=25\ncallbacks=[EarlyStopping(monitor='val_accuracy',min_delta=0.002,patience=3,mode='max',restore_best_weights=True)]\nmodel2_results,model2_df=valuate_model(train_df,\n                                       model2,\n                                       target_column='target',\n                                       use_keyword=True,\n                                       compile_dic=compile_dic,\n                                       batch_size=batch_size,\n                                       epochs=epochs,\n                                       callbacks=callbacks)","metadata":{"id":"eXSX3m21RKMb","outputId":"1763d01b-1f98-4b9c-bd88-3f760f7e7329"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Model 2 got 80.876% score on the test set","metadata":{"id":"M7H8sNU9XdKQ"}},{"cell_type":"markdown","source":"<b>Other Types of Models</b>\n\nNext I will try to use other ML algorithms after fine-tuning the BERT model","metadata":{"id":"jmgPk70lYPKA"}},{"cell_type":"markdown","source":"make the BERT_model un-trainable","metadata":{}},{"cell_type":"code","source":"compile_dic={'optimizer':keras.optimizers.Adam(1e-3,clipvalue=2),\n             'loss':keras.losses.binary_crossentropy,\n             'metrics':['accuracy']}\nBERT_model.trainable=False\nBERT_model.compile(**compile_dic)","metadata":{"id":"VBhxTfFtwNhA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Prepare the data","metadata":{"id":"SBrvEyBqz67k"}},{"cell_type":"code","source":"text=train_df['preprocessed_text'].values\nkeyword_mean=train_df['keyword_mean'].values\nkeyword_mean=keyword_mean.reshape(-1,1)\nprint('text:\\n',text)\nprint('keyword_mean:\\n',keyword_mean)\n\ntext_input=BERT_model.predict(text)\n\nmodel_input=np.concatenate((text_input,keyword_mean.reshape(-1,1)),axis=1)\nprint('model_input:\\n',model_input)\n\nmodel_output=train_df['target'].values\nprint('model_output:\\n',model_output)","metadata":{"id":"EaYj4nx-z9oH","outputId":"1d73cd62-f1fe-421a-da42-35a92bcf3ccc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b>Random Forest</b>","metadata":{"id":"sdZ81RHl3N0T"}},{"cell_type":"code","source":"# forest_classifier=RandomForestClassifier(random_state=random_state,\n#                                          n_jobs=-1)\n# parameters={\n# 'n_estimators':[100,300,500],\n# 'criterion':['gini'],\n# 'max_depth':[10,20,25],\n# 'min_samples_split':[100,200],\n# 'min_samples_leaf':[50,100],\n# 'min_weight_fraction_leaf':[0.0],\n# 'max_leaf_nodes':[20,30]\n# }\n# clf=GridSearchCV(forest_classifier,parameters,verbose=1,cv=5)\n\nclf=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n                       criterion='gini', max_depth=10, max_features='auto',\n                       max_leaf_nodes=20, max_samples=None,\n                       min_impurity_decrease=0.0, min_impurity_split=None,\n                       min_samples_leaf=50, min_samples_split=100,\n                       min_weight_fraction_leaf=0.0, n_estimators=500,\n                       n_jobs=-1, oob_score=False, random_state=27, verbose=0,\n                       warm_start=False)\n\nclf.fit(model_input,model_output)\n\n# best_classifier=clf.best_estimator_\nbest_classifier=clf","metadata":{"id":"mDFiwHrn3LZE","outputId":"00b41b57-1146-4e90-a1f5-e2738f1b3aa5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The grid search took more than 40m","metadata":{"id":"S3IwpeCIJkE-"}},{"cell_type":"code","source":"# clf.best_score_","metadata":{"id":"3T_X83ZC6bsX","outputId":"24728ea5-afde-4522-c1d5-c8fd61ec1c8b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_classifier","metadata":{"id":"P71ufDHSGd-N","outputId":"8758d2b0-1594-4a78-fbed-95fe273e96fb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text=test_df['preprocessed_text'].values\nkeyword_mean=test_df['keyword_mean'].values\nkeyword_mean=keyword_mean.reshape(-1,1)\nprint('text:\\n',text)\nprint('keyword_mean:\\n',keyword_mean)\n\ntext_input=BERT_model.predict(text)\n\ntest_input=np.concatenate((text_input,keyword_mean.reshape(-1,1)),axis=1)\nprint('test_input:\\n',model_input)\n\ntest_df['target']=best_clasifier.predict(test_input).ravel()\ntest_df['target']=test_df['target'].astype(int)\n\ntest_df[['id','target']].set_index('id').to_csv('submission.csv')","metadata":{"id":"m9PSK71_8QFK","outputId":"221225ee-3f97-46f0-bd1d-39e03fe97288"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Random Forest got an 80.723% accuracy, slightly less than Model 2.","metadata":{"id":"CTPqBQ6HHoLn"}},{"cell_type":"markdown","source":"<b>Support Vector Classifier</b>\n\nNow let us try Support Vector classifier (SVC)","metadata":{"id":"DPwLWTazYwuD"}},{"cell_type":"code","source":"# svc_classifier=SVC(random_state=random_state)\n# parameters={\n#     'C':[0.2,0.5,1.0,1.2,1.5],\n#     'kernel':['rbf','sigmoid','linear'],\n#     'degree':[3],\n#     'gamma':['scale','auto'],\n# }\n# clf=GridSearchCV(svc_classifier,parameters,verbose=1,cv=5)\n\nclf=SVC(C=0.2, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n    max_iter=-1, probability=False, random_state=27, shrinking=True, tol=0.001,\n    verbose=False)\n\nclf.fit(model_input,model_output)\n\n# best_classifier=clf.best_estimator_\nbest_classifier=clf","metadata":{"id":"uyHZQYtAH2MM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# clf.best_score_","metadata":{"id":"1b1XPiNeIEru"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_classifier","metadata":{"id":"8qi4xFEYIJ26","outputId":"675764c7-6d9d-4847-b864-d3eab022237f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text=test_df['preprocessed_text'].values\nkeyword_mean=test_df['keyword_mean'].values\nkeyword_mean=keyword_mean.reshape(-1,1)\nprint('text:\\n',text)\nprint('keyword_mean:\\n',keyword_mean)\n\ntext_input=BERT_model.predict(text)\n\ntest_input=np.concatenate((text_input,keyword_mean.reshape(-1,1)),axis=1)\nprint('test_input:\\n',model_input)\n\ntest_df['target']=best_classifier.predict(test_input).ravel()\ntest_df['target']=test_df['target'].astype(int)\n\ntest_df[['id','target']].set_index('id').to_csv('submission.csv')","metadata":{"id":"veCAjgr3IAO_","outputId":"41ad519d-5635-4601-9512-1e3eea7dc65a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"SVC got a slightly better than random forest with 80.907% accuracy. ","metadata":{"id":"On-BhOM5MQAF"}},{"cell_type":"markdown","source":"In the end, the best model was the one used to to fine-tune the BERT model with a score of 81.274% accuracy. The other models were not far oof; they reached around 80% accuracy.","metadata":{"id":"WxMc-XuVZ1lo"}},{"cell_type":"markdown","source":"<h4><b>5-Generate the Results</b></h4>\n    \nThis codes can be used to generate the results for the previous models. The desired model should be trained before running these codes.","metadata":{"id":"LDUjxavpvNhW"}},{"cell_type":"code","source":"print(test_df[test_df['preprocessed_text'].isnull()])\n\ntest_df['preprocessed_text']=test_df['preprocessed_text'].fillna('great')","metadata":{"id":"D6XLsUFb3W2Y","outputId":"ea405b2e-dc08-4eab-d995-a532b0820d6c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print(model.predict(x=[test_df['preprocessed_text'].values,test_df['keyword_mean'].values]))\n\ntest_df['predictions']=(model.predict([test_df['preprocessed_text'].values,test_df['keyword_mean'].values])>0.5).astype(int)","metadata":{"id":"zwmaLEuKpiqg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df[['id','predictions']].set_index('id').rename(columns={'predictions':'target'}).to_csv('submission.csv')","metadata":{"id":"zpVwHdw4ytjH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1><b>Conclusion</b></h1>\n    \nBERT model was fine-tuned on the training data. It used the text preprocessor that comes with the model. aditional information was used from the keyword statistics.<br/>\nMany models were tested. BERT with an additional single output neuron, BERT with 1 hidden layer and 1 output layer, BERT with 1 hidden layer using convolution and 1 output layer, and lastly random forest and support vector machines were used on the BERT generated features and the keyword stats.<br/>\nThe best model was the BERT with 1 output layer. This model achieved 81.274% accuracy.\n\n<h4><b>\nThank You for reading. I hope it was helpful.\n</b></h4>","metadata":{"id":"YAUXYlglMg1t"}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}